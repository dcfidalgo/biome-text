{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a record pair classifier for Customer Data Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.recogn.ai/biome-text/documentation/tutorials/Training_a_record_pair_classifier_for_Customer_Data_Deduplication.html\"><img src=\"https://www.recogn.ai/biome-text/assets/img/biome-isotype.svg\" width=32 />View on recogn.ai</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_record_pair_classifier_for_Customer_Data_Deduplication.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_record_pair_classifier_for_Customer_Data_Deduplication.ipynb\"><img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=32 />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will train a record pair classifier to detect possible duplicates in customer data.\n",
    "\n",
    "The challenge of matching customer records and its deduplication is frequently encountered by big businesses dealing with thousands of customers in their database.\n",
    "A first rough selection of possible matches is often made by [fuzzy matching](https://en.wikipedia.org/wiki/Fuzzy_matching_(computer-assisted_translation)) techniques or the computation of some [string metric](https://en.wikipedia.org/wiki/String_metric) (like the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)).\n",
    "In a second step the fine-grained match often requires human intervention or is made by an AI taking advantage of previous decisions took by humans.\n",
    "\n",
    "This tutorial covers the latter approach for which we us a curated data set by [Uniserv](https://www.uniserv.com/en/), which contains pairs of fictional customer records with a corresponding label.\n",
    "With this data we will train a binary classifier that predicts if two provided records are a duplicate or not. \n",
    "\n",
    "When running this tutorial in Google Colab, make sure to install *biome.text* first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U git+https://github.com/recognai/biome-text.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore warnings and don't forget to restart your runtime afterwards (*Runtime -> Restart runtime*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data before starting with the configuration of our pipeline.\n",
    "For this we create a `DataSource` instance providing a path to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.data import DataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record1</th>\n",
       "      <th>record2</th>\n",
       "      <th>target</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'@gender': 'xxx', '@firstname': 'Werner', '@l...</td>\n",
       "      <td>{'@gender': 'xxx', '@firstname': 'Wernre', '@l...</td>\n",
       "      <td>duplicate</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'@gender': 'Herr', '@firstname': 'Hugo', '@la...</td>\n",
       "      <td>{'@gender': 'Herr', '@firstname': 'Hugo', '@la...</td>\n",
       "      <td>not_duplicate</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'@gender': 'Herr', '@firstname': 'Peter', '@l...</td>\n",
       "      <td>{'@gender': 'Herr', '@firstname': 'Pteer Hans'...</td>\n",
       "      <td>not_duplicate</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'@gender': 'Herr', '@firstname': 'Karl-Heinz'...</td>\n",
       "      <td>{'@gender': 'Herr', '@firstname': 'Karl-Heinz'...</td>\n",
       "      <td>duplicate</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'@gender': 'xxx', '@firstname': 'Walter', '@l...</td>\n",
       "      <td>{'@gender': 'xxx', '@firstname': 'Watler', '@l...</td>\n",
       "      <td>duplicate</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'@gender': 'Frau', '@firstname': 'Lissa', '@l...</td>\n",
       "      <td>{'@gender': 'Frau', '@firstname': 'Lisssa', '@...</td>\n",
       "      <td>duplicate</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'@gender': 'Herr', '@firstname': 'Siegfried',...</td>\n",
       "      <td>{'@gender': 'Herr', '@firstname': 'Siegfriedd'...</td>\n",
       "      <td>duplicate</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'@gender': 'xxx', '@firstname': 'Franz', '@la...</td>\n",
       "      <td>{'@gender': 'xxx', '@firstname': 'Franz', '@la...</td>\n",
       "      <td>not_duplicate</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'@gender': 'Herr', '@firstname': 'Peter', '@l...</td>\n",
       "      <td>{'@gender': 'Herr', '@firstname': 'Peterr', '@...</td>\n",
       "      <td>duplicate</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'@gender': 'Herr', '@firstname': 'Jens', '@la...</td>\n",
       "      <td>{'@gender': 'Herr', '@firstname': 'Jenns', '@l...</td>\n",
       "      <td>duplicate</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             record1  \\\n",
       "0  {'@gender': 'xxx', '@firstname': 'Werner', '@l...   \n",
       "1  {'@gender': 'Herr', '@firstname': 'Hugo', '@la...   \n",
       "2  {'@gender': 'Herr', '@firstname': 'Peter', '@l...   \n",
       "3  {'@gender': 'Herr', '@firstname': 'Karl-Heinz'...   \n",
       "4  {'@gender': 'xxx', '@firstname': 'Walter', '@l...   \n",
       "5  {'@gender': 'Frau', '@firstname': 'Lissa', '@l...   \n",
       "6  {'@gender': 'Herr', '@firstname': 'Siegfried',...   \n",
       "7  {'@gender': 'xxx', '@firstname': 'Franz', '@la...   \n",
       "8  {'@gender': 'Herr', '@firstname': 'Peter', '@l...   \n",
       "9  {'@gender': 'Herr', '@firstname': 'Jens', '@la...   \n",
       "\n",
       "                                             record2         target  \\\n",
       "0  {'@gender': 'xxx', '@firstname': 'Wernre', '@l...      duplicate   \n",
       "1  {'@gender': 'Herr', '@firstname': 'Hugo', '@la...  not_duplicate   \n",
       "2  {'@gender': 'Herr', '@firstname': 'Pteer Hans'...  not_duplicate   \n",
       "3  {'@gender': 'Herr', '@firstname': 'Karl-Heinz'...      duplicate   \n",
       "4  {'@gender': 'xxx', '@firstname': 'Watler', '@l...      duplicate   \n",
       "5  {'@gender': 'Frau', '@firstname': 'Lisssa', '@...      duplicate   \n",
       "6  {'@gender': 'Herr', '@firstname': 'Siegfriedd'...      duplicate   \n",
       "7  {'@gender': 'xxx', '@firstname': 'Franz', '@la...  not_duplicate   \n",
       "8  {'@gender': 'Herr', '@firstname': 'Peterr', '@...      duplicate   \n",
       "9  {'@gender': 'Herr', '@firstname': 'Jenns', '@l...      duplicate   \n",
       "\n",
       "                                                path  \n",
       "0  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "1  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "2  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "3  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "4  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "5  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "6  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "7  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "8  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "9  https://biome-tutorials-data.s3-eu-west-1.amaz...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = DataSource(source=\"https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/record_pair_classifier/train.json\")\n",
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have three relevant columns for our task: *record1*, *record2* and *target*. \n",
    "The *path* column is added automatically by the [DataSource](../../api/biome/text/data/datasource.html#datasource) class to keep track of the source file.\n",
    "\n",
    "Each record is a python dictionary with the field names of the record and their values.\n",
    "It is helpful to mark the field names with a specific token to explicitly separate it from the values for the model.\n",
    "\n",
    "Since the [TaskHead](../../api/biome/text/modules/heads/task_head.html#taskhead) of our model (the [RecordPairClassification](../../api/biome/text/modules/heads/classification/record_pair_classification.html#recordpairclassification)) will expect a *record1*, *record2* and a *label* column to be present in the dataframe, we need to provide a `mapping` dictionary to adjust for the different names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.mapping = {\n",
    "    \"record1\": \"record1\", \n",
    "    \"record2\": \"record2\", \n",
    "    \"label\": \"target\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [DataSource](../../api/biome/text/data/datasource.html#datasource) class stores the data in an underlying [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html) that you can easily access.\n",
    "For example, let's check the size of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or let's check the distribution of our labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "duplicate        5697\n",
       "not_duplicate    4303\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_ds.to_mapped_dataframe().compute()\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your *biome.text* Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical [Pipeline](../../api/biome/text/pipeline.html#pipeline) consists of tokenizing the input, extracting features, applying a language encoding (optionally) and executing a task-specific head in the end.\n",
    "\n",
    "After training a pipeline, you can use it to make predictions or explore the underlying model via the [explore UI](../../documentation/user-guides/02.explore.html).\n",
    "\n",
    "As a first step we must define a configuration for our pipeline. \n",
    "In this tutorial we will create a configuration dictionary and use the `Pipeline.from_config()` method to create our pipeline, but there are [other ways](../../api/biome/text/pipeline.html#pipeline).\n",
    "\n",
    "A *biome.text* pipeline has the following main components:\n",
    "\n",
    "```yaml\n",
    "name: # a descriptive name of your pipeline\n",
    "\n",
    "tokenizer: # how to tokenize the input\n",
    "\n",
    "features: # input features of the model\n",
    "\n",
    "encoder: # the language encoder\n",
    "\n",
    "head: # your task configuration\n",
    "\n",
    "```\n",
    "\n",
    "See the [Configuration section](../../documentation/user-guides/05.configuration.html) for a detailed description of how these main components can be configured.\n",
    "\n",
    "Our complete configuration for this tutorial will be following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {\n",
    "    \"name\": \"uniserv_record_pairs\",\n",
    "    \n",
    "    \"tokenizer\": {\n",
    "        \"text_cleaning\": {\n",
    "            \"rules\": [\"strip_spaces\"]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"features\": {\n",
    "#         \"words\": {\n",
    "#             \"embedding_dim\": 32,\n",
    "#             \"lowercase_tokens\": True,\n",
    "#         },\n",
    "        \"chars\": {\n",
    "            \"embedding_dim\": 64,\n",
    "            \"dropout\": 0.1,\n",
    "            \"encoder\": {\n",
    "                \"type\": \"gru\",\n",
    "                \"hidden_size\": 128,\n",
    "                \"num_layers\": 1,\n",
    "                \"bidirectional\": True,\n",
    "            },\n",
    "            \"lowercase_characters\": True,\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    \"head\": {\n",
    "        \"type\": \"RecordPairClassification\",\n",
    "        \"labels\": list(df.label.value_counts().index),\n",
    "        \"dropout\": 0.1,\n",
    "        \"field_encoder\": {\n",
    "            \"type\": \"gru\",\n",
    "            \"bidirectional\": False,\n",
    "            \"hidden_size\": 64,\n",
    "            \"num_layers\": 1,\n",
    "        },\n",
    "        \"record_encoder\": {\n",
    "            \"type\": \"gru\",\n",
    "            \"bidirectional\": True,\n",
    "            \"hidden_size\": 32,\n",
    "            \"num_layers\": 1,\n",
    "        },\n",
    "        \"matcher_forward\": {\n",
    "            \"is_forward\": True,\n",
    "            \"num_perspectives\": 10,\n",
    "            \"with_full_match\": False,\n",
    "        },\n",
    "        \"matcher_backward\": {\n",
    "            \"is_forward\": False,\n",
    "            \"num_perspectives\": 10,\n",
    "            \"with_full_match\": False,\n",
    "        },\n",
    "        \"aggregator\": {\n",
    "            \"type\": \"gru\",\n",
    "            \"bidirectional\": True,\n",
    "            \"hidden_size\": 32,\n",
    "            \"num_layers\": 1,\n",
    "            \"dropout\": 0.0,\n",
    "        },\n",
    "        \"classifier_feedforward\": {\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_dims\": [32],\n",
    "            \"activations\": [\"relu\"],\n",
    "            \"dropout\": [0.0],\n",
    "        },\n",
    "        \"initializer\": {\n",
    "            \"regexes\": [\n",
    "                [\"_output_layer.weight\", {\"type\": \"xavier_normal\"}],\n",
    "                [\"_output_layer.bias\", {\"type\": \"constant\", \"val\": 0}],\n",
    "                [\".*linear_layers.*weight\", {\"type\": \"xavier_normal\"}],\n",
    "                [\".*linear_layers.*bias\", {\"type\": \"constant\", \"val\": 0}],\n",
    "                [\".*weight_ih.*\", {\"type\": \"xavier_normal\"}],\n",
    "                [\".*weight_hh.*\", {\"type\": \"orthogonal\"}],\n",
    "                [\".*bias.*\", {\"type\": \"constant\", \"val\": 0}],\n",
    "                [\".*matcher.*match_weights.*\", {\"type\": \"kaiming_normal\"}],\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline.from_config(pipeline_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vocabulary\n",
    "\n",
    "Before we can start the training we need to create the vocabulary for our model.\n",
    "For this we define a `VocabularyConfiguration`.\n",
    "\n",
    "In our business name classifier we only want to include words with a general meaning to our word feature vocabulary (like \"Computer\" or \"Autohaus\", for example), and want to exclude specific names that will not help to generally classify the kind of business.\n",
    "This can be achieved by including only the most frequent words in our training set via the `min_count` argument. For a complete list of available arguments see the [VocabularyConfiguration API](../../api/biome/text/configuration.html#vocabularyconfiguration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.configuration import VocabularyConfiguration, WordFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_config = VocabularyConfiguration(sources=[train_ds])#, min_count={WordFeatures.namespace: 5000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass this configuration to our `Pipeline` to create the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/0/data"
     ]
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490bdd4823964182a7abb239578b9542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pl.create_vocabulary(vocab_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the vocabulary we can check the size of our entire model in terms of trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124930"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@@PADDING@@': 0,\n",
       " '@@UNKNOWN@@': 1,\n",
       " 'e': 2,\n",
       " 'r': 3,\n",
       " 't': 4,\n",
       " '@': 5,\n",
       " 'n': 6,\n",
       " 's': 7,\n",
       " 'a': 8,\n",
       " 'i': 9,\n",
       " 'm': 10,\n",
       " 'u': 11,\n",
       " 'h': 12,\n",
       " 'd': 13,\n",
       " 'o': 14,\n",
       " 'l': 15,\n",
       " 'c': 16,\n",
       " 'g': 17,\n",
       " 'b': 18,\n",
       " 'f': 19,\n",
       " 'p': 20,\n",
       " 'z': 21,\n",
       " 'y': 22,\n",
       " 'x': 23,\n",
       " 'k': 24,\n",
       " '3': 25,\n",
       " '4': 26,\n",
       " '5': 27,\n",
       " '7': 28,\n",
       " '6': 29,\n",
       " '2': 30,\n",
       " '1': 31,\n",
       " '.': 32,\n",
       " 'w': 33,\n",
       " '9': 34,\n",
       " '8': 35,\n",
       " '0': 36,\n",
       " '-': 37,\n",
       " 'ü': 38,\n",
       " ',': 39,\n",
       " 'ö': 40,\n",
       " 'j': 41,\n",
       " 'v': 42,\n",
       " 'ß': 43,\n",
       " 'ä': 44,\n",
       " 'q': 45,\n",
       " 'é': 46,\n",
       " 'è': 47,\n",
       " 'ó': 48,\n",
       " 'á': 49,\n",
       " '/': 50,\n",
       " 'í': 51,\n",
       " \"'\": 52}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.backbone.vocab.get_token_to_index_vocabulary(\"char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.configuration import TrainerConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = DataSource(\n",
    "    source=\"https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/record_pair_classifier/valid.json\",\n",
    "    mapping={\"record1\": \"record1\", \"record2\": \"record2\", \"label\": \"target\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfiguration(\n",
    "    optimizer={\n",
    "        \"type\": \"adam\",\n",
    "        \"lr\": 0.002,\n",
    "    },\n",
    "    batch_size=32,\n",
    "    num_epochs=5,\n",
    "    cuda_device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.common.params:validation_dataset_reader = None\n",
      "INFO:allennlp.common.params:train_data_path = output_rpc2/.datasources/training_train.json.yml\n",
      "INFO:allennlp.common.params:validation_data_path = output_rpc2/.datasources/validation_valid.json.yml\n",
      "INFO:allennlp.common.params:test_data_path = None\n",
      "INFO:allennlp.common.params:random_seed = 13370\n",
      "INFO:allennlp.common.params:numpy_seed = 1337\n",
      "INFO:allennlp.common.params:pytorch_seed = 133\n",
      "INFO:allennlp.common.checks:Pytorch version: 1.5.0\n",
      "INFO:allennlp.common.params:trainer.no_grad = ()\n",
      "INFO:allennlp.common.params:trainer.type = gradient_descent\n",
      "INFO:allennlp.common.params:trainer.local_rank = 0\n",
      "INFO:allennlp.common.params:trainer.patience = 2\n",
      "INFO:allennlp.common.params:trainer.validation_metric = -loss\n",
      "INFO:allennlp.common.params:trainer.num_epochs = 5\n",
      "INFO:allennlp.common.params:trainer.cuda_device = 0\n",
      "INFO:allennlp.common.params:trainer.grad_norm = None\n",
      "INFO:allennlp.common.params:trainer.grad_clipping = None\n",
      "INFO:allennlp.common.params:trainer.distributed = None\n",
      "INFO:allennlp.common.params:trainer.world_size = 1\n",
      "INFO:allennlp.common.params:trainer.num_gradient_accumulation_steps = 1\n",
      "INFO:allennlp.common.params:trainer.opt_level = None\n",
      "INFO:allennlp.common.params:trainer.no_grad = None\n",
      "INFO:allennlp.common.params:trainer.learning_rate_scheduler = None\n",
      "INFO:allennlp.common.params:trainer.momentum_scheduler = None\n",
      "INFO:allennlp.common.params:trainer.moving_average = None\n",
      "INFO:allennlp.common.params:trainer.batch_callbacks = None\n",
      "INFO:allennlp.common.params:trainer.epoch_callbacks = None\n",
      "INFO:allennlp.common.util:The following parameters are Frozen (without gradient):\n",
      "INFO:allennlp.common.util:The following parameters are Tunable (with gradient):\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._embedding._module.weight\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.weight_ih_l0\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.weight_hh_l0\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.bias_ih_l0\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.bias_hh_l0\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.common.util:_head._field_encoder._encoder._module._module.weight_ih_l0\n",
      "INFO:allennlp.common.util:_head._field_encoder._encoder._module._module.weight_hh_l0\n",
      "INFO:allennlp.common.util:_head._field_encoder._encoder._module._module.bias_ih_l0\n",
      "INFO:allennlp.common.util:_head._field_encoder._encoder._module._module.bias_hh_l0\n",
      "INFO:allennlp.common.util:_head._record_encoder._module.weight_ih_l0\n",
      "INFO:allennlp.common.util:_head._record_encoder._module.weight_hh_l0\n",
      "INFO:allennlp.common.util:_head._record_encoder._module.bias_ih_l0\n",
      "INFO:allennlp.common.util:_head._record_encoder._module.bias_hh_l0\n",
      "INFO:allennlp.common.util:_head._record_encoder._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.common.util:_head._record_encoder._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.common.util:_head._record_encoder._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.common.util:_head._record_encoder._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.common.util:_head._matcher_forward.maxpool_match_weights\n",
      "INFO:allennlp.common.util:_head._matcher_forward.attentive_match_weights\n",
      "INFO:allennlp.common.util:_head._matcher_forward.max_attentive_match_weights\n",
      "INFO:allennlp.common.util:_head._matcher_backward.maxpool_match_weights\n",
      "INFO:allennlp.common.util:_head._matcher_backward.attentive_match_weights\n",
      "INFO:allennlp.common.util:_head._matcher_backward.max_attentive_match_weights\n",
      "INFO:allennlp.common.util:_head._aggregator._module.weight_ih_l0\n",
      "INFO:allennlp.common.util:_head._aggregator._module.weight_hh_l0\n",
      "INFO:allennlp.common.util:_head._aggregator._module.bias_ih_l0\n",
      "INFO:allennlp.common.util:_head._aggregator._module.bias_hh_l0\n",
      "INFO:allennlp.common.util:_head._aggregator._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.common.util:_head._aggregator._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.common.util:_head._aggregator._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.common.util:_head._aggregator._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.common.util:_head._classifier_feedforward._linear_layers.0.weight\n",
      "INFO:allennlp.common.util:_head._classifier_feedforward._linear_layers.0.bias\n",
      "INFO:allennlp.common.util:_head._output_layer.weight\n",
      "INFO:allennlp.common.util:_head._output_layer.bias\n",
      "INFO:allennlp.common.params:trainer.optimizer.type = adam\n",
      "INFO:allennlp.common.params:trainer.optimizer.parameter_groups = None\n",
      "INFO:allennlp.common.params:trainer.optimizer.lr = 0.002\n",
      "INFO:allennlp.common.params:trainer.optimizer.betas = (0.9, 0.999)\n",
      "INFO:allennlp.common.params:trainer.optimizer.eps = 1e-08\n",
      "INFO:allennlp.common.params:trainer.optimizer.weight_decay = 0.0\n",
      "INFO:allennlp.common.params:trainer.optimizer.amsgrad = False\n",
      "INFO:allennlp.training.optimizers:Number of trainable parameters: 124930\n",
      "INFO:allennlp.common.params:trainer.checkpointer.type = default\n",
      "INFO:allennlp.common.params:trainer.checkpointer.keep_serialized_model_every_num_seconds = None\n",
      "INFO:allennlp.common.params:trainer.checkpointer.num_serialized_models_to_keep = 1\n",
      "INFO:allennlp.common.params:trainer.checkpointer.model_save_interval = None\n",
      "INFO:allennlp.common.params:trainer.tensorboard_writer.summary_interval = 100\n",
      "INFO:allennlp.common.params:trainer.tensorboard_writer.histogram_interval = None\n",
      "INFO:allennlp.common.params:trainer.tensorboard_writer.batch_size_interval = None\n",
      "INFO:allennlp.common.params:trainer.tensorboard_writer.should_log_parameter_statistics = True\n",
      "INFO:allennlp.common.params:trainer.tensorboard_writer.should_log_learning_rate = True\n",
      "INFO:allennlp.common.params:trainer.tensorboard_writer.get_batch_num_total = None\n",
      "INFO:allennlp.training.trainer:Beginning training.\n",
      "INFO:allennlp.training.trainer:Epoch 0/4\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 6737.108\n",
      "INFO:allennlp.training.trainer:Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78da53cc2f4b4843a5a7546a41437f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c21233ec94a4e51b9615a8b3daa943b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.tensorboard_writer:                             Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:macro/fscore             |     0.646  |     0.744\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/not_duplicate    |     0.519  |     0.656\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB            |  6737.108  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/not_duplicate |     0.792  |     0.925\n",
      "INFO:allennlp.training.tensorboard_writer:macro/precision          |     0.729  |     0.826\n",
      "INFO:allennlp.training.tensorboard_writer:loss                     |     0.576  |     0.487\n",
      "INFO:allennlp.training.tensorboard_writer:micro/fscore             |     0.692  |     0.774\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/duplicate        |     0.774  |     0.831\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/duplicate     |     0.666  |     0.727\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/duplicate        |     0.923  |     0.970\n",
      "INFO:allennlp.training.tensorboard_writer:reg_loss                 |     0.000  |     0.000\n",
      "INFO:allennlp.training.tensorboard_writer:macro/recall             |     0.655  |     0.739\n",
      "INFO:allennlp.training.tensorboard_writer:micro/precision          |     0.692  |     0.774\n",
      "INFO:allennlp.training.tensorboard_writer:micro/recall             |     0.692  |     0.774\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy                 |     0.692  |     0.773\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/not_duplicate    |     0.386  |     0.508\n",
      "INFO:allennlp.training.checkpointer:Best validation performance so far. Copying weights to 'output_rpc2/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:02:52.780566\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:11:31\n",
      "INFO:allennlp.training.trainer:Epoch 1/4\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 7350.636\n",
      "INFO:allennlp.training.trainer:Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da9467ad39a48ed86d4f4cf1dd65bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebfb749e6314ad3b7d89933fca85a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.tensorboard_writer:                             Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:macro/fscore             |     0.749  |     0.722\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/not_duplicate    |     0.667  |     0.620\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB            |  7350.636  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/not_duplicate |     0.923  |     0.949\n",
      "INFO:allennlp.training.tensorboard_writer:macro/precision          |     0.825  |     0.830\n",
      "INFO:allennlp.training.tensorboard_writer:loss                     |     0.478  |     0.470\n",
      "INFO:allennlp.training.tensorboard_writer:micro/fscore             |     0.775  |     0.760\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/duplicate        |     0.831  |     0.825\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/duplicate     |     0.728  |     0.711\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/duplicate        |     0.967  |     0.982\n",
      "INFO:allennlp.training.tensorboard_writer:reg_loss                 |     0.000  |     0.000\n",
      "INFO:allennlp.training.tensorboard_writer:macro/recall             |     0.744  |     0.721\n",
      "INFO:allennlp.training.tensorboard_writer:micro/precision          |     0.775  |     0.760\n",
      "INFO:allennlp.training.tensorboard_writer:micro/recall             |     0.775  |     0.760\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy                 |     0.775  |     0.760\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/not_duplicate    |     0.522  |     0.460\n",
      "INFO:allennlp.training.checkpointer:Best validation performance so far. Copying weights to 'output_rpc2/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:01:29.279192\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:06:33\n",
      "INFO:allennlp.training.trainer:Epoch 2/4\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 7350.636\n",
      "INFO:allennlp.training.trainer:Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9861798c68b46d88c1d502da1210bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22785fd87abf4895ad4090d3756f85f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.tensorboard_writer:                             Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:macro/fscore             |     0.748  |     0.747\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/not_duplicate    |     0.666  |     0.661\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB            |  7350.636  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/not_duplicate |     0.912  |     0.922\n",
      "INFO:allennlp.training.tensorboard_writer:macro/precision          |     0.820  |     0.826\n",
      "INFO:allennlp.training.tensorboard_writer:loss                     |     0.456  |     0.430\n",
      "INFO:allennlp.training.tensorboard_writer:micro/fscore             |     0.774  |     0.775\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/duplicate        |     0.829  |     0.832\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/duplicate     |     0.728  |     0.730\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/duplicate        |     0.962  |     0.968\n",
      "INFO:allennlp.training.tensorboard_writer:reg_loss                 |     0.000  |     0.000\n",
      "INFO:allennlp.training.tensorboard_writer:macro/recall             |     0.743  |     0.742\n",
      "INFO:allennlp.training.tensorboard_writer:micro/precision          |     0.774  |     0.775\n",
      "INFO:allennlp.training.tensorboard_writer:micro/recall             |     0.774  |     0.775\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy                 |     0.774  |     0.775\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/not_duplicate    |     0.525  |     0.515\n",
      "INFO:allennlp.training.checkpointer:Best validation performance so far. Copying weights to 'output_rpc2/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:01:28.777810\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:03:53\n",
      "INFO:allennlp.training.trainer:Epoch 3/4\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 7350.636\n",
      "INFO:allennlp.training.trainer:Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58dfaccd7af74420a47e9fe5f0c5f40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc54a29771149788d0f3ae1c2c45ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.tensorboard_writer:                             Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:macro/fscore             |     0.750  |     0.753\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/not_duplicate    |     0.678  |     0.751\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB            |  7350.636  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/not_duplicate |     0.857  |     0.658\n",
      "INFO:allennlp.training.tensorboard_writer:macro/precision          |     0.797  |     0.767\n",
      "INFO:allennlp.training.tensorboard_writer:loss                     |     0.426  |     0.419\n",
      "INFO:allennlp.training.tensorboard_writer:micro/fscore             |     0.771  |     0.754\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/duplicate        |     0.822  |     0.756\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/duplicate     |     0.737  |     0.876\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/duplicate        |     0.929  |     0.665\n",
      "INFO:allennlp.training.tensorboard_writer:reg_loss                 |     0.000  |     0.000\n",
      "INFO:allennlp.training.tensorboard_writer:macro/recall             |     0.745  |     0.769\n",
      "INFO:allennlp.training.tensorboard_writer:micro/precision          |     0.771  |     0.753\n",
      "INFO:allennlp.training.tensorboard_writer:micro/recall             |     0.771  |     0.753\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy                 |     0.770  |     0.753\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/not_duplicate    |     0.560  |     0.873\n",
      "INFO:allennlp.training.checkpointer:Best validation performance so far. Copying weights to 'output_rpc2/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:01:25.440873\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:01:49\n",
      "INFO:allennlp.training.trainer:Epoch 4/4\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 7350.636\n",
      "INFO:allennlp.training.trainer:Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d872cb0b7d47428ddcf057c5d3275f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c203303ebb4461823bc2dcc3aa7580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.tensorboard_writer:                             Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:macro/fscore             |     0.788  |     0.829\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/not_duplicate    |     0.750  |     0.815\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB            |  7350.636  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/not_duplicate |     0.790  |     0.759\n",
      "INFO:allennlp.training.tensorboard_writer:macro/precision          |     0.794  |     0.829\n",
      "INFO:allennlp.training.tensorboard_writer:loss                     |     0.365  |     0.343\n",
      "INFO:allennlp.training.tensorboard_writer:micro/fscore             |     0.795  |     0.830\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/duplicate        |     0.827  |     0.843\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/duplicate     |     0.798  |     0.899\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/duplicate        |     0.857  |     0.794\n",
      "INFO:allennlp.training.tensorboard_writer:reg_loss                 |     0.000  |     0.000\n",
      "INFO:allennlp.training.tensorboard_writer:macro/recall             |     0.785  |     0.836\n",
      "INFO:allennlp.training.tensorboard_writer:micro/precision          |     0.795  |     0.830\n",
      "INFO:allennlp.training.tensorboard_writer:micro/recall             |     0.795  |     0.830\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy                 |     0.795  |     0.830\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/not_duplicate    |     0.713  |     0.879\n",
      "INFO:allennlp.training.checkpointer:Best validation performance so far. Copying weights to 'output_rpc2/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:01:25.578829\n",
      "INFO:allennlp.training.checkpointer:loading best weights\n",
      "INFO:allennlp.models.archival:archiving weights and vocabulary to output_rpc2/model.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pl.train(\n",
    "    output=\"output_rpc2\",\n",
    "    training=train_ds,\n",
    "    validation=valid_ds,\n",
    "    trainer=trainer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
